{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PySpark DataFrame Exercise (Walmart Stock)**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is a hands-on PySpark practice session using a historical Walmart stock dataset (`walmart_stock.csv`).\n",
    "\n",
    "### Goals\n",
    "- Load a CSV file into a Spark DataFrame with inferred schema\n",
    "- Inspect schema and columns (`printSchema`, `columns`, `dtypes`)\n",
    "- Compute descriptive statistics (`describe`)\n",
    "- Answer common analytical questions using core DataFrame operations:\n",
    "  - sorting (`orderBy`)\n",
    "  - filtering (`filter`)\n",
    "  - aggregations (`agg`, `groupBy`)\n",
    "  - correlation (`stat.corr`, `corr`)\n",
    "  - date-based grouping (year / month)\n",
    "\n",
    "### Notes\n",
    "- Spark DataFrames are immutable: transformations return new DataFrames.\n",
    "- Most operations are lazy; actions like `show()` and `count()` trigger computation.\n",
    "- For date-based analysis, the `Date` column should be cast to a proper date type, in case its not correctly infered.\n",
    "\n",
    "---\n",
    "### **Initial Setup and exploration**\n",
    "\n",
    "#### 1. Start a simple Spark Session\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Exercise_1\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load the Walmart Stock CSV File, have Spark infer the data types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = spark.read.csv(\"walmart_stock.csv\", header=True, inferSchema=True)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What are the column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes # This prints the pair of column 'names' and data 'types'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns # This prints only the names of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns: # slightly more natural way of displaying the data\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What does the Schema look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Print out the first 5 columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.columns[:5]).show() # This prints all the data of the slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns[:5]) # This prints only the names of the columns in the slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Use describe() to learn about the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show() # This gives descriptive statistics of all numeric typed columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(df.columns[:3]).show() # This way we can limit the columns summarized, to a slice of the total columns list. It will only summarize numeric types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **SQL Type exploration**\n",
    "\n",
    "#### 7. When was the highest value of High?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use select() and orderBy() when the information in the whole row is important. It is a more expensive operation since it involves sorting.\n",
    "\n",
    "df.select(\"Date\", \"High\").orderBy(df.High.desc()).show(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we only want the value, use agg() There is no sorting so its very quick.\n",
    "df.agg({\"High\": \"max\"}).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What is the mean of the Close column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, round\n",
    "\n",
    "# With the function style, it allows aliases and also to concatenate functions (min, max, etc)\n",
    "df.agg(round(mean(\"Close\"), 2).alias(\"'Close' mean\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This relies on the SQL engine. Fast but the output is not as pretty. No need to import functions.\n",
    "df.agg({\"Close\" : \"mean\"}).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What is the max and min of the Volume column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, round \n",
    "# Use select() when planning to generate new derived columns and reuse that specific calculation.\n",
    "df_MinMeanMax = df.select(min(\"Volume\").alias(\"Min Volume\"),\n",
    "                          max(\"Volume\").alias(\"Max Volume\"),\n",
    "                          round(mean(\"Volume\"), 2).alias(\"Avg Volume\")) # round() takes a float and the number of decimals as arguments.\n",
    "\n",
    "df_MinMeanMax.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use agg() when only looking to summarize. No saved state.\n",
    "df.agg(min(\"Volume\").alias(\"Min Volume\"),\n",
    "       max(\"Volume\").alias(\"Max Volume\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. How many days was the Close lower than 60 dollars?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use select first if you want to retrieve the rows after applying the filter.\n",
    "df.select(\"Date\", \"Close\").filter(df.Close < 60).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This style only cares about the number of records fulfilling the condition. Notice the use of count() instead of show()\n",
    "df.filter(df.Close < 60).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. What percentage of the time was the High greater than 80 dollars ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In other words, (Number of Days High>80)/(Total Days in the dataset)\n",
    "\n",
    "# First create two variables for each part of the operation, similar to making subqueries.\n",
    "grt_80 = df.filter(df.High > 80).count()\n",
    "total_days = df.count()\n",
    "\n",
    "# Either save the result in a new variable or just apply the operation directly while printing.\n",
    "print(f\"Percentage of days where High was over 80 USD: {(grt_80 / total_days) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is the Pearson correlation between High and Volume?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can either use the stat modules corr() function, or import corr() to use directly.\n",
    "# both functions default to Pearson correlation.\n",
    "\n",
    "pearson_corr = df.stat.corr(\"High\", \"Volume\")\n",
    "print(f\"Pearson correlation between High and Volume: {pearson_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "df.agg(round(corr(\"High\", \"Volume\"), 3).alias(\"Corr High vs Volume\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What is the max High per year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import unctions for extracting and aggregating time-based features (day, week, month, year) from date columns in Spark DataFrames\n",
    "from pyspark.sql.functions import (dayofmonth, hour,\n",
    "                                   dayofyear, month,\n",
    "                                   year, weekofyear,\n",
    "                                   format_number, date_format)\n",
    "\n",
    "df.groupBy(year(\"Date\").alias(\"Year\")).agg(max(\"High\").alias(\"Max High\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. What is the average Close for each Calendar Month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In other words, across all the years, what is the average Close price for Jan,Feb, Mar, etc... Your result will have a value for each of these months.\n",
    "\n",
    "from pyspark.sql.functions import avg # This is exactly the same function as mean(), however it helps keep the similiarity with SQL.\n",
    "\n",
    "# Create two new columns Month from existing Date column, one extracting the months number (month()) and one the month name (date_format(\"Date\", \"MMMM\")).\n",
    "# The two columns are useful to display the name, but be able to sort chronologically (using the number), not alphabetically.\n",
    "df_months = df.withColumn(\"MonthNum\", month(\"Date\"))\\\n",
    "              .withColumn(\"Month\", date_format(\"Date\", \"MMMM\"))\n",
    "\n",
    "df_months.groupBy(\"MonthNum\", \"Month\")\\\n",
    "    .agg(avg(\"Close\").alias(\"AVG Close pr Month\"))\\\n",
    "    .orderBy(\"MonthNum\")\\\n",
    "    .select(\"Month\", \"AVG Close pr Month\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
