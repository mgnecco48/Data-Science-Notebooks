{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "329d53ba",
   "metadata": {},
   "source": [
    "# Learning Apache Spark with PySpark\n",
    "*An introduction to basic concepts for working with Apache Spark using Python.*\n",
    "\n",
    "*This notebook implements these concepts in code, serving both as a learning resource and as a collection of reusable code snippets for future projects.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fbfb13",
   "metadata": {},
   "source": [
    "## What is Apache Spark?\n",
    "\n",
    "Apache Spark is a tool designed to process and analyze large amounts of data efficiently.\n",
    "Instead of working on data row by row on a single machine, Spark is built to split work into smaller pieces and process them in parallel.\n",
    "\n",
    "Although Spark is often used on clusters with many machines, it can also run locally on a single computer.\n",
    "In this notebook, Spark is used in **local mode**, which makes it easier to experiment and learn, while still using the same APIs that would later scale to a cluster.\n",
    "\n",
    "At a high level, Spark focuses on:\n",
    "- processing data in parallel,\n",
    "- keeping data in memory when possible for better performance,\n",
    "- delaying execution until a result is actually needed.\n",
    "\n",
    "These ideas make Spark especially useful in Big Data and Cloud Computing contexts, where datasets are too large to be handled efficiently by traditional single-machine tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273fc151",
   "metadata": {},
   "source": [
    "## SparkSession: Entry point to Spark\n",
    "A SparkSession is the main entry point to Apache Spark when using it from Python (PySpark).\n",
    "\n",
    "In practical terms, the SparkSession:\n",
    "connects your Python code to the Spark engine (running on the JVM), manages configuration and resources, allows you to create DataFrames, read data, and run computations.\n",
    "\n",
    "Without a SparkSession, Spark has no context in which to execute your code.\n",
    "\n",
    "You need to import the SparkSession from PySpark in order to start writing any Spark application, then initialize a session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc682cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start the SparkSession,\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Test-1\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e94ef6",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "A **Spark DataFrame** is Spark’s main way of working with structured data.  \n",
    "It looks and feels similar to a Pandas DataFrame, but it is designed to work with much larger datasets.\n",
    "\n",
    "Behind the scenes, a Spark DataFrame:\n",
    "- is split into multiple partitions,\n",
    "- is evaluated lazily (nothing runs until a result is needed),\n",
    "- is optimized automatically by Spark before execution.\n",
    "\n",
    "Because of this, DataFrames are what you’ll use most of the time when working with Spark in PySpark and Spark SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6df4881",
   "metadata": {},
   "source": [
    "### Defining Data\n",
    "\n",
    "To create a DataFrame, two components are required:\n",
    "\n",
    "Data – the actual rows\n",
    "Schema – the structure of the DataFrame (column names and types)\n",
    "\n",
    "This components can be declared in the script, or read from different data files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb85ec",
   "metadata": {},
   "source": [
    "**Declaring the data manually:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95308f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the different entities (rows) of the DataFrame\n",
    "data = [[1, \"Nombre 1\", \"Apellido 1\"],\n",
    "        [2, \"Nombre 2\", \"Apellido 2\"],\n",
    "        [3, \"Nombre 3\", \"Apellido 3\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84e0f8b",
   "metadata": {},
   "source": [
    "**Creating the Schema:**\n",
    "It can be done in different ways, here are two simple ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341eac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the schema for the DataFrame, using SQL DDL\n",
    "schema = \"`ID` INT, `First` STRING, `Second` STRING\"\n",
    "\n",
    "# Create the \"schema\" by naming the columns in another python list\n",
    "columns = [\"ID\", \"Fname\", \"Lname\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d395d6c4",
   "metadata": {},
   "source": [
    "### Creating Spark DataFrames\n",
    "\n",
    "Spark DataFrames are created using the active SparkSession.\n",
    "The same data can produce different DataFrames depending on how the schema is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bdabf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the schema\n",
    "my_first_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# From the columns list\n",
    "my_second_df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d64c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19bb261a",
   "metadata": {},
   "source": [
    "### Displaying the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eec9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the DataFrames\n",
    "my_first_df.show()\n",
    "my_second_df.show()\n",
    "\n",
    "# Method to print the DataFrames schema structures\n",
    "print(my_first_df.printSchema())\n",
    "print(my_second_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6981868f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
