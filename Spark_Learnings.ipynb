{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "329d53ba",
   "metadata": {},
   "source": [
    "# Learning Apache Spark with PySpark\n",
    "*An introduction to basic concepts for working with Apache Spark using Python.*\n",
    "\n",
    "*This notebook implements these concepts in code, serving both as a learning resource and as a collection of reusable code snippets for future projects.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fbfb13",
   "metadata": {},
   "source": [
    "## What is Apache Spark?\n",
    "\n",
    "Apache Spark is a tool designed to process and analyze large amounts of data efficiently.\n",
    "Instead of working on data row by row on a single machine, Spark is built to split work into smaller pieces and process them in parallel.\n",
    "\n",
    "Although Spark is often used on clusters with many machines, it can also run locally on a single computer.\n",
    "In this notebook, Spark is used in **local mode**, which makes it easier to experiment and learn, while still using the same APIs that would later scale to a cluster.\n",
    "\n",
    "At a high level, Spark focuses on:\n",
    "- processing data in parallel,\n",
    "- keeping data in memory when possible for better performance,\n",
    "- delaying execution until a result is actually needed.\n",
    "\n",
    "These ideas make Spark especially useful in Big Data and Cloud Computing contexts, where datasets are too large to be handled efficiently by traditional single-machine tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273fc151",
   "metadata": {},
   "source": [
    "## SparkSession: Entry point to Spark\n",
    "A SparkSession is the main entry point to Apache Spark when using it from Python (PySpark).\n",
    "\n",
    "In practical terms, the SparkSession:\n",
    "connects your Python code to the Spark engine (running on the JVM), manages configuration and resources, allows you to create DataFrames, read data, and run computations.\n",
    "\n",
    "Without a SparkSession, Spark has no context in which to execute your code.\n",
    "\n",
    "You need to import the SparkSession from PySpark in order to start writing any Spark application, then initialize a session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc682cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/26 14:59:38 WARN Utils: Your hostname, MacBook-Air-de-Martin.local, resolves to a loopback address: 127.0.0.1; using 192.168.4.46 instead (on interface en0)\n",
      "26/01/26 14:59:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/26 14:59:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Import the SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start the SparkSession,\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Test-1\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6340e5",
   "metadata": {},
   "source": [
    "## Two Ways of Working with Data in Spark: DataFrames and RDDs\n",
    "\n",
    "Apache Spark offers two main APIs for working with distributed data:\n",
    "\n",
    "- **DataFrames**: high-level, optimized, schema-based (used in practice)\n",
    "- **RDDs (Resilient Distributed Datasets)**: low-level, flexible, conceptual\n",
    "\n",
    "In this notebook, we primarily use **DataFrames**, since they are the modern and recommended approach.\n",
    "However, understanding some RDD concepts helps explain *why* certain DataFrame operations behave the way they do.\n",
    "\n",
    "A useful mental model when working with Spark is to always ask:\n",
    "\n",
    "*“What does **one element** represent at this point in the pipeline?\"*\n",
    "\n",
    "This question applies to both DataFrames and RDDs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e94ef6",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "A **Spark DataFrame** is Spark’s main way of working with structured data.  \n",
    "It looks and feels similar to a Pandas DataFrame, but it is designed to work with much larger datasets.\n",
    "\n",
    "Behind the scenes, a Spark DataFrame:\n",
    "- is split into multiple partitions,\n",
    "- is evaluated lazily (nothing runs until a result is needed),\n",
    "- is optimized automatically by Spark before execution.\n",
    "\n",
    "Because of this, DataFrames are what you’ll use most of the time when working with Spark in PySpark and Spark SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6df4881",
   "metadata": {},
   "source": [
    "### Defining Data\n",
    "\n",
    "To create a DataFrame, two components are required:\n",
    "\n",
    "Data – the actual rows\n",
    "Schema – the structure of the DataFrame (column names and types)\n",
    "\n",
    "This components can be declared in the script, or read from different data files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb85ec",
   "metadata": {},
   "source": [
    "**Declaring the data manually:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95308f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the different entities (rows) of the DataFrame\n",
    "data = [[1, \"Nombre 1\", \"Apellido 1\"],\n",
    "        [2, \"Nombre 2\", \"Apellido 2\"],\n",
    "        [3, \"Nombre 3\", \"Apellido 3\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84e0f8b",
   "metadata": {},
   "source": [
    "**Creating the Schema:**\n",
    "It can be done in different ways, here are two simple ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "341eac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the schema for the DataFrame, using SQL DDL\n",
    "schema = \"`ID` INT, `First` STRING, `Second` STRING\"\n",
    "\n",
    "# Create the \"schema\" by naming the columns in another python list\n",
    "columns = [\"ID\", \"Fname\", \"Lname\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d395d6c4",
   "metadata": {},
   "source": [
    "### Creating Spark DataFrames\n",
    "\n",
    "Spark DataFrames are created using the active SparkSession.\n",
    "The same data can produce different DataFrames depending on how the schema is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19bdabf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the schema\n",
    "my_first_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# From the columns list\n",
    "my_second_df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb261a",
   "metadata": {},
   "source": [
    "### Displaying the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1eec9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+\n",
      "| ID|   First|    Second|\n",
      "+---+--------+----------+\n",
      "|  1|Nombre 1|Apellido 1|\n",
      "|  2|Nombre 2|Apellido 2|\n",
      "|  3|Nombre 3|Apellido 3|\n",
      "+---+--------+----------+\n",
      "\n",
      "+---+--------+----------+\n",
      "| ID|   Fname|     Lname|\n",
      "+---+--------+----------+\n",
      "|  1|Nombre 1|Apellido 1|\n",
      "|  2|Nombre 2|Apellido 2|\n",
      "|  3|Nombre 3|Apellido 3|\n",
      "+---+--------+----------+\n",
      "\n",
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Second: string (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Fname: string (nullable = true)\n",
      " |-- Lname: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Printing the DataFrames\n",
    "my_first_df.show()\n",
    "my_second_df.show()\n",
    "\n",
    "# Method to print the DataFrames schema structures\n",
    "print(my_first_df.printSchema())\n",
    "print(my_second_df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac07de4",
   "metadata": {},
   "source": [
    "## RDDs: Conceptual Foundation\n",
    "\n",
    "Although this notebook primarily uses **DataFrames**, it is useful to briefly understand\n",
    "**RDDs (Resilient Distributed Datasets)**, as they form the conceptual foundation of Spark.\n",
    "\n",
    "RDDs are managed by the **SparkContext**, which is already available through the\n",
    "`SparkSession`. This is why RDDs are created via:\n",
    "\n",
    "\n",
    "```spark.sparkContext```\n",
    "\n",
    "An RDD cannot be created directly like a Python list, because it represents a\n",
    "distributed collection that requires Spark’s execution engine, scheduler, and lineage tracking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba23795",
   "metadata": {},
   "source": [
    "### Creating an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6981868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([\n",
    "    \"This is a line\",\n",
    "    \"Another line\"\n",
    "])\n",
    "# At this point, each element of the RDD is one line (a string).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42600f4f",
   "metadata": {},
   "source": [
    "### `map` vs `flatMap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef21d70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This', 'is', 'a', 'line'], ['Another', 'line']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using map\n",
    "rdd_map = rdd.map(lambda line: line.split())\n",
    "rdd_map.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9545bd",
   "metadata": {},
   "source": [
    "Here:\n",
    "\n",
    "- each input element (a line) produces one output element\n",
    "\n",
    "- the output elements are lists of words\n",
    "\n",
    "- So the RDD contains lists, not individual words.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a22ac70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'line', 'Another', 'line']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using flatMap\n",
    "rdd_flatmap = rdd.flatMap(lambda line: line.split())\n",
    "rdd_flatmap.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0231ca5e",
   "metadata": {},
   "source": [
    "Here:\n",
    "\n",
    "- one input element produces multiple output elements\n",
    "\n",
    "- the lists are flattened\n",
    "\n",
    "- each RDD element is now one word\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9779ff63",
   "metadata": {},
   "source": [
    "## Text Processing with DataFrames\n",
    "\n",
    "In this section, we read and process a text file using **DataFrames**.\n",
    "Although DataFrames are used for the implementation, the operations closely\n",
    "relate to the RDD concepts introduced earlier.\n",
    "\n",
    "---\n",
    "\n",
    "### Reading a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f74e86fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+\n",
      "|value                                                                   |\n",
      "+------------------------------------------------------------------------+\n",
      "|Apache Spark is a powerful engine for large-scale data processing.      |\n",
      "|Spark allows data to be processed in parallel across multiple machines. |\n",
      "|This notebook explores how Spark handles text data using DataFrames.    |\n",
      "|Understanding how data is transformed helps avoid common mistakes.      |\n",
      "|Spark provides both low-level and high-level APIs for working with data.|\n",
      "+------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.text(\"text_example.txt\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d3dd68",
   "metadata": {},
   "source": [
    "When reading a text file with spark.read.text:\n",
    "\n",
    "- Each row represents one line of the file\n",
    "- In this case, the DataFrame contains a single column, usually named value\n",
    "\n",
    "This is analogous to an RDD where each element corresponds to one line.\n",
    "\n",
    "At this stage, the mental model is:\n",
    "\n",
    ">*one row = one line of text*\n",
    "\n",
    "---\n",
    "\n",
    "### Splitting lines into words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d6ceae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|       word|\n",
      "+-----------+\n",
      "|     Apache|\n",
      "|      Spark|\n",
      "|         is|\n",
      "|          a|\n",
      "|   powerful|\n",
      "|     engine|\n",
      "|        for|\n",
      "|large-scale|\n",
      "|       data|\n",
      "|processing.|\n",
      "|      Spark|\n",
      "|     allows|\n",
      "|       data|\n",
      "|         to|\n",
      "|         be|\n",
      "|  processed|\n",
      "|         in|\n",
      "|   parallel|\n",
      "|     across|\n",
      "|   multiple|\n",
      "+-----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "words_df = df.select(\n",
    "    explode(split(df.value, \" \")).alias(\"word\")\n",
    ")\n",
    "\n",
    "words_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61628b35",
   "metadata": {},
   "source": [
    "Here, two operations are combined:\n",
    "\n",
    "- `split` transforms each line into a list of words\n",
    "\n",
    "- `explode` flattens these lists so that each word becomes its own row\n",
    "\n",
    "Conceptually:\n",
    "\n",
    "- `split` produces a structure similar to `map(lambda line: line.split())`\n",
    "\n",
    "- `explode` plays a role similar to `flatMap`, flattening the result\n",
    "\n",
    "After this step, the mental model becomes:\n",
    "\n",
    ">*one row = one word*\n",
    "\n",
    "---\n",
    "\n",
    "### Counting words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "778385c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       word|count|\n",
      "+-----------+-----+\n",
      "|  mistakes.|    1|\n",
      "|    handles|    1|\n",
      "|      using|    1|\n",
      "|        for|    2|\n",
      "|        how|    2|\n",
      "|   provides|    1|\n",
      "|   powerful|    1|\n",
      "|      data.|    1|\n",
      "|         in|    1|\n",
      "|       with|    1|\n",
      "|         be|    1|\n",
      "|  processed|    1|\n",
      "|  machines.|    1|\n",
      "|       both|    1|\n",
      "|     Apache|    1|\n",
      "|     allows|    1|\n",
      "|         is|    2|\n",
      "|   parallel|    1|\n",
      "|DataFrames.|    1|\n",
      "|       data|    4|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "word_counts = words_df.groupBy(\"word\").count()\n",
    "word_counts.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f163f",
   "metadata": {},
   "source": [
    "This groups identical words and counts how many times each appears.\n",
    "\n",
    "Because DataFrames are schema-based and optimized, Spark can:\n",
    "\n",
    "- reorder operations\n",
    "\n",
    "- reduce data movement\n",
    "\n",
    "- execute the aggregation efficiently\n",
    "\n",
    "This is one of the main advantages of using DataFrames over RDDs for\n",
    "structured operations like counting and grouping.\n",
    "\n",
    "---\n",
    "\n",
    "**Important note on actions**\n",
    "\n",
    "Operations such as `show()` trigger execution of the Spark pipeline.\n",
    "\n",
    "Unlike RDDs:\n",
    "\n",
    "- DataFrames provide `show()` for inspection\n",
    "\n",
    "- RDDs use `collect()` to return data as a local Python list\n",
    "\n",
    "In both cases, these operations should only be used on small datasets\n",
    "for debugging or demonstration purposes.\n",
    "\n",
    "---\n",
    "\n",
    "## Text Processing with RDDs (MapReduce Mental Model)\n",
    "\n",
    "To clearly contrast DataFrames with RDDs, we now perform the same text processing task\n",
    "using RDDs. This helps illustrate the classic *MapReduce-style workflow* and makes the\n",
    "differences between the two APIs explicit.\n",
    "\n",
    "### Reading the text file as an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "543ed1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile(\"text_example.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46f5e24",
   "metadata": {},
   "source": [
    "At this point:\n",
    "\n",
    "- each RDD element is one line of text\n",
    "\n",
    "- this is conceptually equivalent to `spark.read.text(...)`\n",
    "\n",
    "Before any transformations are applied:\n",
    "\n",
    "> *one RDD element = one line*\n",
    "\n",
    "---\n",
    "\n",
    "### Splitting lines into words (flatMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50578da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'a',\n",
       " 'powerful',\n",
       " 'engine',\n",
       " 'for',\n",
       " 'large-scale',\n",
       " 'data',\n",
       " 'processing.',\n",
       " 'Spark',\n",
       " 'allows',\n",
       " 'data',\n",
       " 'to',\n",
       " 'be',\n",
       " 'processed',\n",
       " 'in',\n",
       " 'parallel',\n",
       " 'across',\n",
       " 'multiple',\n",
       " 'machines.',\n",
       " 'This',\n",
       " 'notebook',\n",
       " 'explores',\n",
       " 'how',\n",
       " 'Spark',\n",
       " 'handles',\n",
       " 'text',\n",
       " 'data',\n",
       " 'using',\n",
       " 'DataFrames.',\n",
       " 'Understanding',\n",
       " 'how',\n",
       " 'data',\n",
       " 'is',\n",
       " 'transformed',\n",
       " 'helps',\n",
       " 'avoid',\n",
       " 'common',\n",
       " 'mistakes.',\n",
       " 'Spark',\n",
       " 'provides',\n",
       " 'both',\n",
       " 'low-level',\n",
       " 'and',\n",
       " 'high-level',\n",
       " 'APIs',\n",
       " 'for',\n",
       " 'working',\n",
       " 'with',\n",
       " 'data.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd = rdd.flatMap(lambda line: line.split())\n",
    "words_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90045b6a",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- `line.split()` produces a list of words per line\n",
    "\n",
    "- `flatMap` flattens these lists into individual elements\n",
    "\n",
    "Mental model:\n",
    "\n",
    "> *one RDD element = one word*\n",
    "\n",
    "This corresponds directly to:\n",
    "\n",
    "`split` + `explode` in the DataFrame \n",
    "\n",
    "---\n",
    "\n",
    "### Mapping words to key–value pairs (map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbe2a218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apache', 1),\n",
       " ('Spark', 1),\n",
       " ('is', 1),\n",
       " ('a', 1),\n",
       " ('powerful', 1),\n",
       " ('engine', 1),\n",
       " ('for', 1),\n",
       " ('large-scale', 1),\n",
       " ('data', 1),\n",
       " ('processing.', 1),\n",
       " ('Spark', 1),\n",
       " ('allows', 1),\n",
       " ('data', 1),\n",
       " ('to', 1),\n",
       " ('be', 1),\n",
       " ('processed', 1),\n",
       " ('in', 1),\n",
       " ('parallel', 1),\n",
       " ('across', 1),\n",
       " ('multiple', 1),\n",
       " ('machines.', 1),\n",
       " ('This', 1),\n",
       " ('notebook', 1),\n",
       " ('explores', 1),\n",
       " ('how', 1),\n",
       " ('Spark', 1),\n",
       " ('handles', 1),\n",
       " ('text', 1),\n",
       " ('data', 1),\n",
       " ('using', 1),\n",
       " ('DataFrames.', 1),\n",
       " ('Understanding', 1),\n",
       " ('how', 1),\n",
       " ('data', 1),\n",
       " ('is', 1),\n",
       " ('transformed', 1),\n",
       " ('helps', 1),\n",
       " ('avoid', 1),\n",
       " ('common', 1),\n",
       " ('mistakes.', 1),\n",
       " ('Spark', 1),\n",
       " ('provides', 1),\n",
       " ('both', 1),\n",
       " ('low-level', 1),\n",
       " ('and', 1),\n",
       " ('high-level', 1),\n",
       " ('APIs', 1),\n",
       " ('for', 1),\n",
       " ('working', 1),\n",
       " ('with', 1),\n",
       " ('data.', 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_pairs = words_rdd.map(lambda word: (word, 1))\n",
    "word_pairs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8725c35",
   "metadata": {},
   "source": [
    "Here:\n",
    "\n",
    "- each word is mapped to a tuple (word, 1)\n",
    "\n",
    "- this prepares the data for aggregation\n",
    "\n",
    "This step represents the ***Map*** phase in ***MapReduce***.\n",
    "\n",
    "---\n",
    "\n",
    "### Reducing by key (counting words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de0e8e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('powerful', 1),\n",
       " ('for', 2),\n",
       " ('to', 1),\n",
       " ('parallel', 1),\n",
       " ('multiple', 1),\n",
       " ('machines.', 1),\n",
       " ('explores', 1),\n",
       " ('how', 2),\n",
       " ('handles', 1),\n",
       " ('text', 1),\n",
       " ('using', 1),\n",
       " ('Understanding', 1),\n",
       " ('transformed', 1),\n",
       " ('helps', 1),\n",
       " ('avoid', 1),\n",
       " ('low-level', 1),\n",
       " ('and', 1),\n",
       " ('working', 1),\n",
       " ('with', 1),\n",
       " ('data.', 1),\n",
       " ('Apache', 1),\n",
       " ('Spark', 4),\n",
       " ('is', 2),\n",
       " ('a', 1),\n",
       " ('engine', 1),\n",
       " ('large-scale', 1),\n",
       " ('data', 4),\n",
       " ('processing.', 1),\n",
       " ('allows', 1),\n",
       " ('be', 1),\n",
       " ('processed', 1),\n",
       " ('in', 1),\n",
       " ('across', 1),\n",
       " ('This', 1),\n",
       " ('notebook', 1),\n",
       " ('DataFrames.', 1),\n",
       " ('common', 1),\n",
       " ('mistakes.', 1),\n",
       " ('provides', 1),\n",
       " ('both', 1),\n",
       " ('high-level', 1),\n",
       " ('APIs', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts_rdd = word_pairs.reduceByKey(lambda a, b: a + b)\n",
    "word_counts_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb3c0a2",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- all values associated with the same key (word) are combined\n",
    "\n",
    "- the values (1) are summed\n",
    "\n",
    "This represents the ***Reduce*** phase in ***MapReduce***.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a79666",
   "metadata": {},
   "source": [
    "## Comparison with the DataFrame approach\n",
    "\n",
    "When working with **RDDs**, the full logic of the computation is written out step by step:\n",
    "\n",
    "- each transformation is explicit and low-level  \n",
    "- we manually define how data is mapped and reduced  \n",
    "- Spark executes the provided functions as **black boxes**, without knowing their intent  \n",
    "\n",
    "This gives a lot of control, but also requires more code and makes optimization harder.\n",
    "\n",
    "When working with **DataFrames**, the focus shifts from *how* to compute the result to *what* result we want:\n",
    "\n",
    "- operations such as `groupBy` and `count` describe the intent declaratively  \n",
    "- Spark can analyze the operations and choose an efficient execution plan  \n",
    "- the resulting code is usually shorter and easier to read  \n",
    "\n",
    "Both approaches produce the same final result.  \n",
    "However, DataFrames allow Spark to apply automatic optimizations, which is why they are generally preferred in practice.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
